{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs(requests.get(\"https://timesofindia.indiatimes.com\").text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headings = soup.find_all('div', class_ = 'top-story')\n",
    "print(headings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = html_string.find_all('a')\n",
    "linkFile = list()\n",
    "for link in links:\n",
    "    if link['href'].startswith('https'):\n",
    "        linkFile.append(link['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for heading in headings:\n",
    "    h1 = heading.find_all('li')\n",
    "#print(h1)\n",
    "h2 = list()\n",
    "for h in h1:\n",
    "    h2.append(h.a['title'].strip())\n",
    "for h in h2:\n",
    "    print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " import csv\n",
    "with open('D:\\\\dpa\\link.txt', 'a+', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "h = pd.DataFrame(h2)\n",
    "h.to_csv('D:\\\\dpa\\link.csv', header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkFile = pd.DataFrame(linkFile)\n",
    "linkFile.to_csv('D:\\\\dpa\\linkFile.csv', header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = soup.find_all('div', class_ = 'latestNewContainer')\n",
    "for i in top:\n",
    "    list1 = i.find_all('li')\n",
    "#print(list1)\n",
    "for l in list1:\n",
    "    #if l.has_attr('title'):\n",
    "    h2.append(l.a['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "head2 = (soup.find_all('h2'))\n",
    "for h in head2:\n",
    "    if h.text:\n",
    "        if h.a.string:\n",
    "            print(h.a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head2 = (soup.find_all('h2'))\n",
    "topHead = list()\n",
    "for h in head2:\n",
    "    if h:\n",
    "        topHead.append(h.text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topHead = pd.DataFrame(topHead)\n",
    "topHead.to_csv('D:\\\\dpa\\headings.csv', header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = soup.find_all(\"ul\", class_ = 'list9')\n",
    "for i in news:\n",
    "    news1 = i.find_all('li')\n",
    "\n",
    "for i in news1:\n",
    "    temp1=(i.text.strip())\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxml.html.clean as clean\n",
    "safe_attrs = set(['src', 'alt', 'href', 'title', 'width', 'height'])\n",
    "kill_tags = ['object', 'iframe']\n",
    "cleaner = clean.Cleaner(safe_attrs_only=True, safe_attrs=safe_attrs, kill_tags=kill_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html1 = bs(requests.get(\"https://timesofindia.indiatimes.com/\").text,'html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxml.html.clean as clean\n",
    "safe_attrs = set(['src', 'alt', 'href', 'title', 'width', 'height'])\n",
    "kill_tags = ['object', 'iframe']\n",
    "cleaner = clean.Cleaner(safe_attrs_only=True, safe_attrs=safe_attrs, kill_tags=kill_tags)\n",
    "html_string = html1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newhead = html_string.find_all(\"ul\", class_ = 'list9')\n",
    "ls = list()\n",
    "for i in newhead:\n",
    "    new = i.find_all('a')\n",
    "for i in new:\n",
    "    ls.append(i.text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ls)):\n",
    "    if ls[i] != '' and i != 4:\n",
    "        h2.append(ls[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(h2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
